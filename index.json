[{"authors":null,"categories":null,"content":"\nI‚Äôm a data scientist working in the financial services industry and this is my website! I‚Äôm passionate about using the power of mathematics, statistics, data and technology to solve complex problems and improve lives. I‚Äôm a life-long learner and enjoy sharing what Im passionate about with others. Throughout the site, you‚Äôll find projects I‚Äôve worked on, tools I‚Äôve built, blog posts on topics related to data science, machine learning \u0026amp; AI and more information about yours truly.\n  Download my resum√©.\n","date":1562889600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1562889600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://bobbylumpkin.com/author/bobby-lumpkin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bobby-lumpkin/","section":"authors","summary":"I‚Äôm a data scientist working in the financial services industry and this is my website! I‚Äôm passionate about using the power of mathematics, statistics, data and technology to solve complex problems and improve lives.","tags":null,"title":"Bobby Lumpkin","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Bobby Lumpkin FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://bobbylumpkin.com/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, \u0026#39;Hello world\u0026#39;]  Tuples\n Tuples are immutable - they can‚Äôt be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, \u0026#39;Hello world\u0026#39;)   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://bobbylumpkin.com/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026#34;country == \u0026#39;Canada\u0026#39;\u0026#34;) fig = px.bar(data_canada, x=\u0026#39;year\u0026#39;, y=\u0026#39;pop\u0026#39;) fig.show() \n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://bobbylumpkin.com/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://bobbylumpkin.com/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://bobbylumpkin.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"Beating the Vig with AWS Arbitrage is a term that originates from the world of finance and refers to the practice of taking advantage of price discrepancies in different markets to guarantee a risk-free profit. At its core, it involves buying an asset in one market where the price is low and simultaneously selling it in another market where the price is higher. This principle relies on the assumption that markets are not perfectly efficient, and temporary mispricing can occur. For example, if a stock is trading for 100 dollars on one exchange and 102 dollars on another, an arbitrageur could buy the stock on the cheaper exchange and sell it on the more expensive one, pocketing the 2 dollar difference, all while assuming little to no risk.\nNow, imagine applying this concept to sports betting, a world where odds vary across different sportsbooks. We can exploit misaligned prices across different markets just the same. Namely, we can identify mismatched odds between different bookmakers and place multiple bets on the same event, guaranteeing a profit regardless of the outcome.\nIn this post, I‚Äôll walk through the details of sports-betting arbitrage as well as discuss how I architected an automated system to scan for these opportunities across multiple sportsbooks, alerting subscribers when they arise.\n\rTable of Contents  Beating the Vig with AWS  Table of Contents Sports Betting: Fundamentals  Decimal Odds Fractional Odds American Odds   Sports Betting: Arbitrage  Implied Probabilities Necessary \u0026amp; Sufficient Conditions for Arbitrage   Architecting An Arbitrage Notification System in AWS  Lambda: Query API Lambda: Search for Arbitrage Opportunities Lambda: Publish New Opportunities Step Function: Orchestrating the Workflow Simple Notification Service (SNS): Pub/Sub   Conclusion    \rSports Betting: Fundamentals When it comes to sports betting, odds are the key factor in determining potential payouts. They‚Äôre related to the probability of a particular outcome and dictate how much money you can win if that outcome occurs. Books differ in how they convey odds, but three popular formats appear most frequently: decimal, fractional, and American.\nDecimal Odds Decimal odds are commonly used in Europe, Canada and Australia. They represent the total payout (stake \u0026amp; profit) for every dollar wagered. The formula to calculate your total payout is:\n$$ \\text{Payout} = \\text{Stake} \\times \\text{Odds}. $$\nFor example, if the odds for a bet are $2.50$ and we bet $10$ dollars, our payout will be:\n$$ \\text{Payout} = 10 \\times 2.50 = 25, $$\nmeaning that we would win $15$ dollars in profit.\nFractional Odds More common in the UK, fractional odds are presented as a fraction, like $5/1$ or $10/3$. The numerator represents the amount of profit you will make for every unit staked, while the denominator represents the stake required to win that profit. The formula to calculate the profit is:\n$$ \\text{Profit} = \\text{Stake} \\times \\text{Odds}. $$\nFor instance, with $10/3$ odds, if we bet $60$ dollars, our profit would be:\n$$ \\text{Profit} = 60 \\times \\left(\\frac{10}{3}\\right) = 200. $$\nThus, we would receive a total payout of $260$ dollars ($200$ dollar profit plus our $60$ dollar stake).\nAmerican Odds As is American tradition, American Odds (also referred to as ‚ÄúMoneyline Odds‚Äù) are often found to be the least intuitive and straightforward units of measurement for new-comers. Popular in the United States, they can either be positive or negative and revolve around a $100$ dollar baseline. Positive odds (assigned to the underdog) show how much profit we would make on a $100$ dollar bet, while negative odds (assigned to the favorite) show how much we need to bet in order to win $100$ dollars.\n  Positive Odds (e.g. $+200$): This means we make a profit of $200$ dollars for every $100$ wagered and:\n$$ \\text{Profit} = \\frac{\\text{Stake} \\times 200}{100}. $$\nFor example, a $50$ dollar bet on $+200$ odds would give\n$$ \\text{Profit} = \\frac{50 \\times 200}{100} = 100. $$\n  Negative Odds (e.g. $-150$): This means we need to wager $150$ dollars in order to win $100$ dollars and:\n$$ \\text{Profit} = \\frac{\\text{Stake}}{|-150|} \\times 100. $$\nFor example, a $75$ dollar bet on $-150$ odds would yield:\n$$ \\text{Profit} = \\frac{75}{150} \\times 100 = 50. $$\n  Remembering how to read and interpret the different conventions for odds formatting may well be the most irritating part of the sports-betting initiation process and to muddy the waters even further: for those those statistically-minded individuals out there, you‚Äôll notice that none of these formats directly mirrors what we refer to formally as odds, in the technical sense. Only out of due-diligence did we present three formats here; we‚Äôll choose one to work with throughout the rest of the post. While Moneyline odds are common in th US, we will not be utilizing them nor fractional odds. For their simplicity and convenience with calculations in future sections, we will adopt decimal odds as our convention. ‚Ä¶","date":1754179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1754179200,"objectID":"bcdfe09e0b27bf7f1978627a73288d70","permalink":"https://bobbylumpkin.com/project/sports-betting-arbitrage-system/","publishdate":"2025-08-03T00:00:00Z","relpermalink":"/project/sports-betting-arbitrage-system/","section":"project","summary":"An automated system to alert users when arbitrage opportunities arise across sports-betting books.","tags":["Data Science","Lambda Functions","Step Functions","SNS","Automation","AWS"],"title":"Beating the Vig with AWS","type":"project"},{"authors":null,"categories":null,"content":"Overview and Table of Contents With the shattering of performance benchmarks and the development of applications like ChatGPT and Bing Chat, lots of time and writing has been dedicated to demystifying advancements in neural network architectures. But advancements in tokenization, a necessary preprocessing step often take place with less publicity. In this post, we‚Äôll lend a little love and spotlight to the topic. We‚Äôll review what tokenization is, why it‚Äôs done and walk through the details behind both simple and more complex, recent developments and approaches.\n Tokenization: What It Is and Why It‚Äôs Done Word \u0026amp; Subword Level Methods  Word-Level Methods Subword-Level Methods  Byte Pair Encoding WordPiece     Byte-Level Byte Pair Encoding References  \rTokenization: What It Is and Why It‚Äôs Done    Tokenization is the splitting up of a text into some concept of ‚Äòatomic‚Äô units: words or sub-word units, for example. In order to utilize advancements in neural networks for natural language processing (NLP), we first need a way to convert text into a machine learning digestible format. Tokenization is the first step in this process; it allows us to break a text into contributing components. Those components are then typically mapped to vectors, through an embedding layer, and can be fed into a neural network.\nHow text should be decomposed isn‚Äôt obvious and lots of differing approaches exist. The example pictured above is a simple example: namely, splitting at every space character. While such methods do have use-cases, many modern NLP pipelines also utilize more sophisticated methods. We‚Äôll touch on some of these in the upcoming sections, looking at methods on the word, subword and byte levels, culminating with an introduction to the approach used in models like RoBERTa and GPT 3.5 \u0026amp; 4.\nWord \u0026amp; Character Level Methods Word-Level Methods Why not separate characters by spaces? Let‚Äôs think about what typically happens after tokenization, namely, embedding. For every token in our vocabulary, we‚Äôll generate\u0026#39; an embedding vector. That means that the size of our embedding matrix or model will scale with the size of our vocabulary. You‚Äôll notice that, in splitting by spaces, punctuation gets lumped in with words. Do we really want to include all of ‚Äúshare‚Äù, ‚Äúshare!‚Äù, ‚Äúshare?‚Äù and any other punctuation combinations in the corpus as separate tokens? Not dealing with punctuation could lead to massive inefficiencies.\nAs an improvement, we could try to additionally split around punctuation. In other words, our initial example would get tokenized as\nimport re TEXT = \u0026#34;Rico, Barry and Alexandra have something to share!\u0026#34; re.findall( r\u0026#39;\\w+|[^\\s\\w]+\u0026#39;, TEXT) [\u0026#39;Rico\u0026#39;,\u0026#39;,\u0026#39;,\u0026#39;Barry\u0026#39;,\u0026#39;and\u0026#39;,\u0026#39;Alexandra\u0026#39;,\u0026#39;have\u0026#39;,\u0026#39;something\u0026#39;,\u0026#39;to\u0026#39;,\u0026#39;share\u0026#39;,\u0026#39;!\u0026#39;] A different example might highlight some lingering deficiencies, though.\nTEXT = \u0026#34;I don\u0026#39;t wanna keep secrets just to keep you.\u0026#34; re.findall( r\u0026#39;\\w+|[^\\s\\w]+\u0026#39;, TEXT) [\u0026#34;I\u0026#34;, \u0026#34;don\u0026#34;,\u0026#34;\u0026#39;\u0026#34;,\u0026#34;t\u0026#34;,\u0026#34;wanna\u0026#34;,\u0026#34;keep\u0026#34;,\u0026#34;secrets\u0026#34;,\u0026#34;just\u0026#34;,\u0026#34;to\u0026#34;,\u0026#34;keep\u0026#34;,\u0026#34;you\u0026#34;,\u0026#34;.\u0026#34;] There‚Äôs something unintuitive about the way ‚Äúdon‚Äôt‚Äù was split. We know ‚Äúdon‚Äôt‚Äù stands for ‚Äúdo not‚Äù and so, it might be better to tokenize it as\n[\u0026#34;I\u0026#34;,\u0026#34;do\u0026#34;,\u0026#34;n\u0026#39;t\u0026#34;,\u0026#34;wanna\u0026#34;,\u0026#34;keep\u0026#34;,\u0026#34;secrets\u0026#34;,\u0026#34;just\u0026#34;,\u0026#34;to\u0026#34;,\u0026#34;keep\u0026#34;,\u0026#34;you\u0026#34;,\u0026#34;.\u0026#34;] Here‚Äôs another one:\nTEXT = \u0026#34;The Labour party currently holds 199 seats in the U.K. parliament.\u0026#34; re.findall( r\u0026#39;\\w+|[^\\s\\w]+\u0026#39;, TEXT) [\u0026#34;The\u0026#34;, \u0026#34;Labour\u0026#34;,\u0026#34;party\u0026#34;,\u0026#34;currently\u0026#34;,\u0026#34;holds\u0026#34;,\u0026#34;199\u0026#34;,\u0026#34;seats\u0026#34;,\u0026#34;in\u0026#34;,\u0026#34;the\u0026#34;,\u0026#34;U\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;K\u0026#34;,\u0026#34;.\u0026#34;,\u0026#34;parliament\u0026#34;,\u0026#34;.\u0026#34;] We know that the ‚ÄúU.K.‚Äù is an abbreviation, representing a single entity so splitting it by punctuation doesn‚Äôt make much sense. What we‚Äôre getting at with these examples is that there are additional rules and rule-exceptions that a simple punctuation and/or space based approach doesn‚Äôt take into account.\nWith that in mind, there are a number of popular rule-based, word-level tokenizers, like those in spaCy that can accomodate for these exceptions and others. In short, spaCy‚Äôs documentation describes their tokenization algorithms broadly as first splitting on white space and then, processing from left to right, performing the two following checks:\n Does the substring match a tokenizer exception rule? Can a prefix, suffix or infix be split off?     Visual example of spaCy tokenization.\nRevisiting the last code example, we can use spaCy‚Äôs rule-based tokenizer as follows:\nimport spacy TEXT = \u0026#34;The Labour party currently holds 199 seats in the U.K. parliament.\u0026#34; nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) doc = nlp(TEXT) print([token.text for token in doc]) [\u0026#39;The\u0026#39;,\u0026#39;Labour\u0026#39;,\u0026#39;party\u0026#39;,\u0026#39;currently\u0026#39;,\u0026#39;holds\u0026#39;,\u0026#39;199\u0026#39;,\u0026#39;seats\u0026#39;,\u0026#39;in\u0026#39;,\u0026#39;the\u0026#39;,\u0026#39;U.K.\u0026#39;,\u0026#39;parliament\u0026#39;,\u0026#39;.\u0026#39;] Subword-Level Methods In some cases, subword-level tokenization offers potential advantages. This is especially true when the text corpus is large, the language model has a fixed size vocabulary and/or handling of rare or novel words (relative to the training set) is necessary (either as input or for generation). In fact, because of the revitalized modern interest in subword-level tokenization, ‚Ä¶","date":1697241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697241600,"objectID":"88daed02c1dad6c0a48426ba9876c37c","permalink":"https://bobbylumpkin.com/project/tokenization/","publishdate":"2023-10-14T00:00:00Z","relpermalink":"/project/tokenization/","section":"project","summary":"An Introduction to Tokenization methods in modern NLP.","tags":["LLMs","Machine Learning","NLP","tokenize","Deep Learning"],"title":"Make it 'Byte'-Sized","type":"project"},{"authors":null,"categories":null,"content":"Overview and Table of Contents Large Langage Models (or LLMs) are all the rage these days. From AI assistants that can write our emails and do our homework for us to reinvigorated conversations about AGI and what it means to be conscious, LLMs have truly transitioned to center stage in the year 2023. Heck, LLMs can even write blog post introductions: ‚ÄúIn this post, we‚Äôll look at an example of how LLMs can be leveraged to extract knowledge, glean insights and engage interactive dialogues with repositories of information, such as academic papers and legal documents.‚Äù. Get ready to chat with your documents!\n Introduction to LLM Pipelines Chaining Frameworks: Haystack \u0026amp; LangChain  Haystack  LangChain Compare \u0026amp; Contrast   Pipeline Components: Putting the Pieces Together  Loading Documents Document Splitters Words to Vectors: Embeddings \u0026amp; Vectorstores  Bag of Words Term Frequency - Inverse Document Frequency Word Embeddings   Retrievers  Similarity Search Locality Sensitive Hashing   Rankers: Bi-Encoders vs Cross-Encoders Readers Generative Pipelines   LLM Pipeline Use-Cases CLI Application References  \rIntroduction to LLM Pipelines Computers represent information as yesses and nos, 1s and 0s, but most human beings don‚Äôt speak binary to each other every day. How, then, do people communicate with computers? From low-level languages like assembly to high-level abstractions like Python, computer science has produced a number of different paradigms that enable just that. We call them programming languages. But the extent to which programming languages enable ‚Äúcommunication‚Äù is restricted. Namely, communications to the machine are required to follow strict rules (syntax) in order to be understood properly (interpreted or compiled) and any communication from the machine must first have been explicitly instructed by the programmer. So, how on earth do applications like ChatGPT exist?!\nEnter natural language processing (NLP) and LLM pipelines. The goal of NLP is to endow a machine with the ability to understand and utilize natural language, that is: the means by which we communicate with other people (plain old English, for example). This is a tall ask, but modern frameworks like LangChain and Haystack put it within reach for those with some Python experience and minimal exposure to concepts in modern NLP. In the following sections, we‚Äôll explore some foundational NLP \u0026amp; LLM concepts, along with Haystack and LangChain building blocks, all directed towards the aim of developing applications that enable us to chat with our documents. Everything we do is based on open source technologies and can be run entirely locally. That means, no sending your data to APIs and no paywalls!\n\rChaining Frameworks: Haystack \u0026amp; LangChain It‚Äôs never been easier and never been less intimidating to build powerful NLP applications. Haystack and LangChain are part of a new wave of tools for interfacing with LLMs and related infrastructure that have made great advancements in abstraction. The rest of this post will be a mixture of high-level concepts, a little bit of theory and examples of how to implement those theories and concepts using these frameworks. So, let‚Äôs familiarize ourselves a bit.\nHaystack Haystack describes itself as an ‚Äúopen source Python framework by deepset for building custom apps with large language models ‚Äú. It utilizes three main types of objects:\n Components or Nodes: ‚Äúfundamental building blocks that can perform tasks like document retrieval, text generation, or summarization.‚Äù Pipelines: ‚Äústructures made up of components, such as a Retriever and Reader, connected to infrastructure building blocks, such as a DocumentStore (for example, Elasticsearch or Weaviate) to form complex systems.‚Äù Agents: makes use of tools (like pipeline components or whole pipelines) to resolve complex tasks.     For our purposes, we‚Äôll need only the first two. We‚Äôll talk more about each object we use as we come to it, but you can find, still, more info in Haystack‚Äôs documentation.\nLangChain Similar to Haystack, LangChain describes itself as ‚Äúa framework for developing applications powered by language models.‚Äù And the similarities don‚Äôt end there. They share very similar interfaces, as well. The analogous objects here are:\n Components: ‚Äúabstractions for working with language models, along with a collection of implementations for each abstraction.‚Äù Chains: ‚Äúa structured assembly of components for accomplishing specific higher-level tasks.‚Äù Agents: lets ‚Äúchains choose which tools to use given high-level directives.‚Äù  Again, we‚Äôll only be utilizing the first two, with more implementation details to come. Checkout LangChain‚Äôs documentation for more information.\n   Compare \u0026amp; Contrast Two frameworks with similar functionality and similar interfaces ‚Ä¶ naturally, the questions arise:\n ‚ÄúIs one ‚Äòbetter‚Äô than the other?‚Äù ‚ÄúWhich should we use?‚Äù and ‚ÄúWhy are we using both?‚Äù.  If you like straight answers, apologies in advance because my honest takes are:\n ‚ÄúIt ‚Ä¶","date":1692144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692144000,"objectID":"34e92b5496f9426b5877d4d1ebab418b","permalink":"https://bobbylumpkin.com/project/docs2chat/","publishdate":"2023-08-16T00:00:00Z","relpermalink":"/project/docs2chat/","section":"project","summary":"A CLI application for chatting with your documents, powered by Haystack and LangChain.","tags":["LLMs","Machine Learning","NLP","LangChain","Haystack","Deep Learning","AWS"],"title":"Docs2Chat","type":"project"},{"authors":["Boseung Choi","Sydney Busch","Dieudonne Kazadi","Benoit Kebela","Emile Okitolonda","Yi Dai","Robert M Lumpkin","Wasiur Rahman Khuda Bukhsh","Omar Saucedo","Marcel Yotebieng","Joe Tien","Eben B Kenah","Grzegorz A Rempala"],"categories":null,"content":" Click the Cite button above to view import publication metadata.   ","date":1644710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644710400,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://bobbylumpkin.com/publication/journal-article/","publishdate":"2022-02-13T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Math","Statistics","Epidemiology"],"title":"Modeling outbreak data: Analysis of a 2012 Ebola virus disease epidemic in DRC","type":"publication"},{"authors":null,"categories":null,"content":"COMING SOON\n","date":1644710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644710400,"objectID":"71b26696d03e01c335ea75e63ac1115c","permalink":"https://bobbylumpkin.com/project/twitter-sentiment-analysis/","publishdate":"2022-02-13T00:00:00Z","relpermalink":"/project/twitter-sentiment-analysis/","section":"project","summary":"Build a web app, interact with twitter's API and build a full sentiment analysis pipeline, all in python.","tags":["Machine Learning","NLP","AWS"],"title":"Twitter Sentiment Analysis","type":"project"},{"authors":["Bobby Lumpkin"],"categories":null,"content":"COMING SOON\n","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"cee0d5c95dcf3e8f46cd5fe95cce9b81","permalink":"https://bobbylumpkin.com/post/weapons-math-destruction/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/weapons-math-destruction/","section":"post","summary":"COMING SOON","tags":["Math","Statistics","Data Ethics"],"title":"\"Weapons of Math Destruction\" Prompted Discussion \u0026 A Review","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne \r**Two** \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://bobbylumpkin.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://bobbylumpkin.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]